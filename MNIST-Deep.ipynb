{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用TensorFlow 之前，首先导入它\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 为了高效地在Python 里进行数值计算，我们一般会使用像NumPy 这样用其他语言\n",
    "# 编写的库件，在Python 外用其它执行效率高的语言完成这些高运算开销操作（如矩阵\n",
    "# 运算）。但是，每一步操作依然会需要切换回Python 带来很大开销。特别的，这种开销\n",
    "# 会在GPU 运算或是分布式集群运算这类高数据传输需求的运算形式上非常高昂。\n",
    "\n",
    "# TensorFlow 将高运算量计算放在Python 外进行，同时更进一步设法避免上述的额\n",
    "# 外运算开销。不同于在Python 中独立运行运算开销昂贵的操作，TensorFlow 让我们可\n",
    "# 以独立于Python 以外以图的形式描述交互式操作。这与Theano、Torch 的做法很相似。\n",
    "\n",
    "# 我们先从创建输入图像和输出类别的节点来创建计算图。\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "# 这里的x和y并不代表具体值，他们是一个占位符(placeholder) — 当TensorFlow 运\n",
    "# 行时需要赋值的变量。\n",
    "\n",
    "# 虽然placeholder的shape参数是可选的，但有了它，TensorFlow 能够自动捕捉因数据\n",
    "# 维度不一致导致的错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable需要在session之前初始化，才能在session中使用。初始化需要初始值（本\n",
    "# 例当中是全为零）传入并赋值给每一个Variable。这个操作可以一次性完成。\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 预测分类与损失函数\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 在训练中最小化损失函数同样很简单。我们这里的损失函数用目标分类和模型预\n",
    "# 测分类之间的交叉熵。\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 我们已经定义好了模型和训练的时候用的损失函数，接下来使用TensorFlow 来训\n",
    "# 练。因为TensorFlow 知道整个计算图，它会用自动微分法来找到损失函数对于各个变\n",
    "# 量的梯度。TensorFlow 有大量内置优化算法，这个例子中，我们用快速梯度下降法让交\n",
    "# 叉熵下降，步长为0.01。\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_step这个操作，用梯度下降来更新权值。因此，整个模型的训练可以通过反\n",
    "# 复地运行train_step来完成。\n",
    "for i in range(1000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "# 每一步迭代，我们都会加载50 个训练样本，然后执行一次train_step，使用feed_dict\n",
    "# ，用训练数据替换placeholder向量x和y_。注意，在计算图中，你可以用feed_dict来替代\n",
    "# 任何张量，并不仅限于替换placeholder。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9092\n"
     ]
    }
   ],
   "source": [
    "# 评估模型\n",
    "# 首先，要先知道我们哪些label 是预测正确了。tf.argmax是一个非常有用的函数。它\n",
    "# 会返回一个张量某个维度中的最大值的索引。例如，tf.argmax(y,1)表示我们模型对每\n",
    "# 个输入的最大概率分类的分类值。而tf.argmax(y_,1)表示真实分类值。我们可以用tf.\n",
    "# equal来判断我们的预测是否与真实分类一致。\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "# 这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数\n",
    "# 来代表对、错，然后取平均值。例如：[True, False, True, True]变为[1,0,1,1]，计算出\n",
    "# 平均值为0.75\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\"))\n",
    "\n",
    "print accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
